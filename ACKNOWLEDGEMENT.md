## Image-Text Data
* [**LLaVA-OneVision**](https://llava-vl.github.io/blog/2024-08-05-llava-onevision/), [**Cambrian-1**](https://github.com/cambrian-mllm/cambrian), [**Pixmo**](https://github.com/allenai/molmo)
* [**Coyo-700M**](https://github.com/kakaobrain/coyo-dataset), [**Object365**](https://www.objects365.org/overview.html), [**SA-1B**](https://ai.meta.com/datasets/segment-anything/), [**BLIP3-OCR**](https://huggingface.co/datasets/Salesforce/blip3-ocr-200m)
* [**DenseFusion-1M**](https://github.com/baaivision/DenseFusion), [**Pixel Parsing**](https://huggingface.co/pixparse)
* [**ShareGPT4o**](https://sharegpt4o.github.io/), [**ShareGPT4V**](https://sharegpt4v.github.io/)
## Video-Text Data
* [**LLaVA-Video**](https://llava-vl.github.io/blog/2024-09-30-llava-video/)
## Text Data
* [**Magpie**](https://github.com/magpie-align/magpie)
